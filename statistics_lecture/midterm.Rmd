---
title: "2022-1 통계학 특강 중간시험"
output: html_notebook
---

### r 코드를 채운 후 학번.rmd로 저장하여 이루리에 제출하세요.


먼저 `faraway`라이브러리를 불러오세요.

```{r}
library(faraway)

```

우리는 `faraway`데이터 안에 있는 `fat` 데이터를 이용할거에요. `fat`데이터의 구조를 살펴보기 위해 처음 6행을 출력해볼게요. 그리고 `fat`데이터의 차원과 `fat`데이터에 있는 변수들의 이름을 살펴봅시다.

```{r}
head(fat)
dim(fat)
names(fat)
```

`fat`데이터의 `brozek`, `siri`, `density` 변수는 서로 수학적 관계가 있는 변수들이에요. 추가적으로 `free`변수 또한 `weight`변수와 `density`변수 등과 연관이 깊습니다. 우리는 이 자료에서 `siri`를 반응변수로 한 모형을 설정해 볼거에요. 따라서 쉬운 분석을 위해 `fat`데이터에서 `brozek`,`density`, `free` 변수가 빠진 `fat1`데이터를 새롭게 만드세요. 참고로 `brozek`변수는 `fat`데이터의 1번째 변수, `density`변수는 3번째 변수, 그리고 `free`변수는 8번째 변수에요! 


```{r}


fat1 <- fat[,-c(1,3,8)]
summary(fat1)
```

`fat1`을 훈련자료 `fat.train`과 시험자료 `fat.test`로 나눠봅시다. seed는 100으로 하고, 훈련자료는 200개, 시험자료는 52개를 랜덤으로 추출하여 각각 `fat.train`과 `fat.test`에 저장하세요. 앞으로의 분석을 용이하게 하기위해 설명변수들에 대해 표준화도 해줍시다. 예를들어 `fat.train`을 표준화시키려면 `fat.train[,-1]<-scale(fat.train[,-1])`과 같이 수행하면 됩니다. 

```{r}
set.seed(100)
ind <- sample(1:nrow(fat1), 200)

fat.train <- fat1[ind,]
fat.test <- fat1[-ind,]
fat.train[,-1] <- scale(fat.train[,-1])
fat.test[,-1] <- scale(fat.test[,-1])
```


단계적 변수선택법을 수행하기 위해 `leaps` 라이브러리에 있는 `regsubsets()` 함수를 이용할거에요. `fwd`라는 변수명으로 종속변수는 `siri`로한 전향 단계적 변수 선택 방법을 수행하세요. 물론 데이터는 앞에서 생성한 `fat.train`을 이용합니다. 단, 최대 설명변수의 수를 14개로 합니다. `fwd.summ`에는 `fwd`의 요약정보를 저장하세요. 

```{r}
library(leaps)
fwd <- regsubsets(siri ~ ., data = fat.train,
    nvmax = 14, method = "forward")
fwd.summ <- summary(fwd)
fwd.summ
```

$C_p$, BIC를 최소로 하고 조정된 $R^2$값이 최대가 되게하는 설명변수의 갯수는 각각 어떻게 되나요? 각각 `cp.min`, `bic.min`, `adjr2.max`에 저정해보세요. 

```{r}


cp.min<-which.min(fwd.summ$cp)

bic.min<-which.min(fwd.summ$bic)


adjr2.max<-which.max(fwd.summ$adjr2)

```

하나의 창에 세 개의 그림을 그려봐요. 각각의 그림은 $x$를 설명변수의 수, $y$를 "Cp", "BIC", "조정된 R^2"으로 하여 두 변수의 관계를 나타내는 곡선이 그려져야 하며, 각 통계량이 $C_p$와 BIC는최소가 되게 하는 지점, 그리고 조정된 $R^2$이 최대가 되는 지점에는 빨간색 점이 찍혀있게 하세요. 

```{r}
plot(fwd.summ$cp, xlab = "Number of Variables",
    ylab = "Cp", type = "l")
points(cp.min, fwd.summ$cp[7], col = "red", cex = 2,
    pch = 20)

plot(fwd.summ$bic, xlab = "Number of Variables",
    ylab = "BIC", type = "l")
points(bic.min, fwd.summ$bic[4], col = "red", cex = 2,
    pch = 20)

plot(fwd.summ$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l")
points(11, fwd.summ$adjr2[10], col = "red", cex = 2, 
    pch = 20)
```

BIC를 최소로 하는 설명변수의 갯수를 사용한 모형들과 조정된 $R^2$을 최소로하는 설명변수의 갯수를 사용한 모형들의 시험평균제곱오차를 시험자료를 이용하여 구해보고 비교해보세요. C_p를 최소로 하는 설명변수의 갯수를 사용한 모형들의 시험평균제곱오차는 이렇게 구합니다. 
```{r}
coef.bic<-coef(fwd,bic.min)
testx = model.matrix(siri~., fat.test)
pred.bic<-testx[,names(coef.bic)]%*%coef.bic
mse.bic<-mean((fat.test$siri-pred.bic)^2)
mse.bic

coef.adjr2<-coef(fwd,adjr2.max)
testx = model.matrix(siri~., fat.test)
pred.adjr2<-testx[,names(coef.adjr2)]%*%coef.adjr2
mse.adjr2<-mean((fat.test$siri-pred.adjr2)^2)
mse.adjr2

```
어떤 기준으로 설명변수의 갯수를 사용한 모형의 평균제곱오차가 가장 작은가요?
```{r}
bic로 했을때 가장 작다.
```


이제 교차 검증을 사용하여 다양한 크기의 모형 들을 비교-선택해보겠습니다.  먼저, 각 관측치를 `k=10` 단(fold) 중 하나에 할당하는 벡터`folds`를 만들고, 그 결과를 저장할 행렬 `cv.errors`을 만들어 보세요. seed 번호는 1을 사용하세요.

```{r}
k <- 10
n <- nrow(fat1)
set.seed(1)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 14,
    dimnames = list(NULL, paste(1:14)))
```


교차 검증을 수행하는 `for` 루프를 작성해봅시다. j번째 단에서는 j와 동일한 단이 검증자료에 있고 나머지는 훈련자료에 있습니다. 수업시간에 만든 `predict.regsubsets()` 함수를 사용하여 각 모형 크기에서의 예측을 하고, 적절한 하위 집합에 대한 검증오차를 계산한 다음 행렬 `cv.errors`의 해당 슬롯에 저장합니다. 

```{r}
 predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}

for (j in 1:k) {
  best.fit <- regsubsets(siri ~ .,
       data = fat1[folds != j, ],
       nvmax = 14)
  for (i in 1:14) {
    pred <- predict(best.fit, fat1[folds == j, ], id = i)
    cv.errors[j, i] <-
         mean((fat1$siri[folds == j] - pred)^2)
   }
 }

```


`apply()`함수를 이용하여 최적 $i$-변수 모형의 교차검증 오차를 계산하고, 그 결과를 그려봅시다. 교차검증 오차가 최소가 되는 지점에는 빨간색 점이 찍혀있게 하세요. 
 
```{r}

```



이제 능형회귀와 라쏘를 적용해보겠습니다. 먼저 `y`에 반응변수인 `siri`를 저장하고, `x`에는 설명변수 행렬을 저장합시다. 단, 절편은 포함되지 않도록 합니다. `x`의 처음 6행도 확인해보세요. 물론 데이터는 `fat.train`입니다.

```{r}

```

능형회귀의 적합을 위해 `glmnet` 패키지를 불러옵시다. 그리고 후보 $\lambda$ 값으로 $10$부터 $-10$까지 동일한 간격으로 이루어진 수열을 `exp(1)`의 거듭제곱으로 한 값을 고려하겠습니다 (즉, $e^{a_1},e^{a_2}, \dots, e^{a_{100}}$).`ridge`에 능형회귀 결과를 저장하세요.

```{r}

```

$\log \lambda$를 $x$축으로한 정규화 그림을 그려봅시다. 단순히 옵션을 `xvar='lambda' `를 주면 될거에요.
```{r}

```
교차검증을 통해 최적의 $\lambda$를 선택해봅시다 seed번호를 1번으로하고 10단 교차검증을 수행해서 `cv.ridge`에 저장하세요. 교차검증오차가 최소가 되게 하는 $\lambda$를 `bestlam`에 저장하세요.

```{r}

```

시험자료`fat.test`의 `bestlam`에서의 평균제곱오차를 구해보세요.

```{r}

```
이제 라쏘에 대해서도 위의 절차를 반복해봅시다. 먼저 `lasso`에 적합결과를 저장합시다.

```{r}

```


$\log \lambda$를 $x$축으로한 정규화 그림을 그려봅시다. 단순히 옵션을 `xvar='lambda' `를 주면 될거에요.

```{r}

```

교차검증을 통해 최적의 $\lambda$를 선택해봅시다 seed번호를 1번으로하고 10단 교차검증을 수행해서 `cv.lasso`에 저장하세요. 교차검증오차가 최소가 되게 하는 $\lambda$를 `bestlam.lasso`에 저장하세요.


```{r}

```


$\lambda$=`bestlam.lasso`에서의 변수선택 결과는 어떻게 되나요? `predict` 함수와 `type="coefficients"` 인수를 활용하면 살펴볼 수 있을거에요. 

```{r}

```

시험자료`fat.test`의 `bestlam.lasso`에서의 평균제곱오차를 구해보세요.

```{r}

```


